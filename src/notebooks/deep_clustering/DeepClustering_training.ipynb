{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the deep clustering monaural source separation model\n",
    "\n",
    "This notebook contains a detailed example of how to train the deep clustering source separation model.  Filepaths to load training data must be filled in to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting imports\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "fig_size = [0,0]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Import the deep clustering separation model\n",
    "from magnolia.dnnseparate.deep_clustering_model import DeepClusteringModel\n",
    "\n",
    "# Import utilities for using the model\n",
    "from magnolia.utils.clustering_utils import clustering_separate, get_cluster_masks, process_signal\n",
    "from magnolia.features.supervised_iterator import SupervisedIterator, SupervisedMixer\n",
    "from magnolia.features.hdf5_iterator import SplitsIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "    numsources  : Number of sources used in training mixes\n",
    "    batchsize   : Number of examples per batch used in training\n",
    "    datashape   : (Time, Frequency) shape of the examples within each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numsources = 2\n",
    "batchsize = 256\n",
    "datashape = (40, fft_size//2 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data I/O\n",
    "\n",
    "For training, only the training dataset is needed.  The other two datasets can be used for evaluation.  The (training set, or in set) speaker keys have been separated according to speaker gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libritrain = \"Path to training dataset\"\n",
    "\n",
    "with open('Magnolia/data/librispeech/authors/train-clean-100-F.txt','r') as speakers:\n",
    "    keys = speakers.read().splitlines()\n",
    "    speaker_keys = keys[:]\n",
    "    in_set_F = keys[:]\n",
    "    \n",
    "with open('Magnolia/data/librispeech/authors/train-clean-100-M.txt','r') as speakers:\n",
    "    keys = speakers.read().splitlines()\n",
    "    speaker_keys += keys\n",
    "    in_set_M = keys[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an mixer that iterates over examples from the training set. \n",
    "\n",
    "SplitsIterator handles (deterministically) splitting the training set into three partitions.  80% of the training data is used to train the model, 10% is used to evaluate the training progress on unseen examples, and the last 10% is reserved to evaluate the performance of the model on unseen examples from speakers in the training set.\n",
    "\n",
    "SupervisedMixer handles the mixing of training examples. It outputs the model input (X), the output labels (Y) and the speakerIDs (I) of the speakers who are loudest in each time frequency bin.  Y must be reshaped and transposed so that it has shape (batchsize,time,frequency,numspeakers).\n",
    "\n",
    "Scaling of the mixtures to create input batches for the model is done here as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the splits iterator\n",
    "siterator = SplitsIterator([0.8,0.1,0.1], libritrain, speaker_keys=speaker_keys, shape=datashape, return_key=True)\n",
    "siterator.set_split(0)\n",
    "\n",
    "# Create the data mixer\n",
    "mixer = SupervisedMixer([siterator,siterator], shape=datashape, \n",
    "                        mix_method='add', diffseed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model expects a different form of label than Lab41's model, some helper functions can be used to convert the output of the mixer into what the model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_batch(mixer, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch from the mixer\n",
    "    \"\"\"\n",
    "    batch = mixer.get_batch(batch_size, out_TF=None)\n",
    "    return batch\n",
    "\n",
    "def gen_batch(mixer,batch_size):\n",
    "    \"\"\"\n",
    "    Create a batch from the mixer of the specified size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a batch of mixed examples\n",
    "    batch = gen_train_batch(mixer, batch_size)\n",
    "    \n",
    "    # Scale the input spectrograms\n",
    "    X = np.sqrt(np.abs(batch[0]))\n",
    "    X = (X - X.min())/(X.max() - X.min())\n",
    "    \n",
    "    # Convert the labels given by the mixer to the form the deep clustering model expects\n",
    "    y = 1/2*(batch[1] + 1)\n",
    "    y = y.reshape(batch_size, 2, T, F)\n",
    "    y = y.transpose(0,2,3,1)\n",
    "    \n",
    "    return X, y, phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generate some validation data\n",
    "To generate a batch from the validation split of the training dataset, the splits iterator can have the split set to the validation split and the mixer can be used as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the current split to the validation split\n",
    "siterator.set_split(1)\n",
    "\n",
    "# Generate a batch of validation data\n",
    "X_vala, y_vala, phases = gen_batch(mixer, batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the deep clustering model\n",
    "\n",
    "Here an untrained model instance is created, and its variables are initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DeepClusteringModel()\n",
    "model.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables needed to track the training progress of the model\n",
    "\n",
    "During training, the number of iterations (number of processed batches) is tracked, along with the mean cost on examples from the training data and from the validation data.  The last iteration that the model was saved on can also be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = []\n",
    "costs = []\n",
    "\n",
    "t_costs = []\n",
    "v_costs = []\n",
    "\n",
    "last_saved = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Here the model is iteratively trained on batches generated by the mixer.  The model is saved every time the validation cost reaches a new minimum value.  The training can be configured to stop if the model has not been saved after a specified number of iterations have elapsed since the previous save.  Plots of the training cost and the validation set are created as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations to train for (should be large)\n",
    "num_iterations = 1000000\n",
    "# Threshold for stopping if the model hasn't improved for this many consecutive iterations\n",
    "stop_threshold = 10000\n",
    "\n",
    "# Find the number of iterations already elapsed (Useful for resuming training)\n",
    "if len(iterations) == 0:\n",
    "    start = 0\n",
    "else:\n",
    "    start = iterations[-1]\n",
    "\n",
    "# Ensure that the iterator is set to iterate over the training split\n",
    "siterator.set_split(0)\n",
    "\n",
    "# Iterate over training batches\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Generate a batch of training data\n",
    "    Xdata, Ydata, _ = gen_batch(mixer, batchsize)\n",
    "    \n",
    "    # Train the model on one batch and get the cost\n",
    "    c = model.train_on_batch(Xin,Ydata,Idata)\n",
    "\n",
    "    # Store the training cost\n",
    "    costs.append(c)\n",
    "    \n",
    "    # Every 10 batches, evaluate the model on the validation data and plot the cost curves\n",
    "    if (i+1) % 10 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        \n",
    "        # Get the cost on the validation batch\n",
    "        c_v = model.get_cost(X_vala, y_vala)\n",
    "        \n",
    "        # Check if the validation cost is below the minimum validation cost, and if so, save it.\n",
    "        if c_v < min(v_costs) and len(iterations) > 0:\n",
    "            print(\"Saving the model because c_v is\", min(v_costs) - c_v, \"below the old min.\")\n",
    "            \n",
    "            # Save the model to the specified path\n",
    "            model.save(\"Path to saved model\")\n",
    "            \n",
    "            # Record the iteraion that the model was last saved on\n",
    "            last_saved = iterations[-1]\n",
    "        \n",
    "        # Store the training cost and the validation cost\n",
    "        t_costs.append(np.mean(costs))\n",
    "        v_costs.append(c_v)\n",
    "        \n",
    "        # Store the current iteration number\n",
    "        iterations.append(i + 1 + start)\n",
    "        \n",
    "        # Compute scale quantities for plotting\n",
    "        length = len(iterations)\n",
    "        cutoff = int(0.5*length)\n",
    "        lowline = [min(v_costs)]*len(iterations)\n",
    "        \n",
    "        # Generate the plots and show them\n",
    "        f, (ax1, ax2) = plt.subplots(2,1)\n",
    "        \n",
    "        ax1.plot(iterations,t_costs)\n",
    "        ax1.plot(iterations,v_costs)\n",
    "        ax1.plot(iterations,lowline)\n",
    "        \n",
    "        y_u = max(max(t_costs[cutoff:]),max(v_costs[cutoff:]))\n",
    "        y_l = min(min(t_costs[cutoff:]),min(v_costs[cutoff:]))\n",
    "        \n",
    "        ax2.set_ylim(y_l,y_u)\n",
    "        \n",
    "        ax2.plot(iterations[cutoff:], t_costs[cutoff:])\n",
    "        ax2.plot(iterations[cutoff:], v_costs[cutoff:])\n",
    "        ax2.plot(iterations[cutoff:], lowline[cutoff:])\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Cost on iteration\", iterations[-1], \"is\", c_v)\n",
    "        print(\"Last saved\",iterations[-1]-last_saved,\"iterations ago.\")\n",
    "        \n",
    "        # Reset the cost over the last 10 iterations\n",
    "        costs = []\n",
    "        \n",
    "        # Stop training if the number of iterations since the last save point exceeds the threshold\n",
    "        if iterations[-1]-last_saved > stop_threshold:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow1.1",
   "language": "python",
   "name": "tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
